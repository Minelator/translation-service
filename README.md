## Привет, я АПИ.

Я завернута в докер контейнер, так что можешь работать со мной через Docker Desktop!

## Потанцуем?

Чтобы начать, запусти эту команду в корневой директории проекта:

`docker-compose up --build`

В одном из контейнеров находится апи сервер. Вся информация о нём в директории [/api-service](/api-service).

К нему можно обратиться по `http://localhost:8000`, чтобы узнать, запущен ли сервер.

Чтобы пообщаться с нейросетью, нужно совершить POST-запрос следующего типа:
```
curl -X POST http://localhost:8000/generate \
     -H "Content-Type: application/json" \
     -d '{
            "prompt": "Hello! Nice to meet you.", 
            "model": "qwen:0.5b"
        }'`
```
Здесь в prompt вписывается сам запрос, а в model — имя модели, с которой хочется поговорить.
Сервер ollama сам понимает, к какой модели обращаться (по переданному имени), если модель уже установлена.

## А если я хочу другую модель?

Молодец, что спросил!
У тебя есть два пути:

1. Изменить контейнер так, чтобы при билде модель скачивалась автоматически.
2. Добавлять новые модели руками.

Для выполнения `пункта 1` нужно зайти в [/ollama/entrypoint.sh](/ollama/entrypoint.sh) и поменять имя модели в `ollama pull qwen:0.5b` на любую необходимую. Потом `docker-compose up --build`.

Для выполнения `пункта 2` нужно зайти в `Docker Desktop` > `translation-service` > контейнер `ollama_server` > вкладка `Exec` и пишем `ollama pull <имя_модели>`.

## Docker классный.

Запускаются контейнеры по `docker-compose up`, выключаются по `docker-compose down`. Эти операции выполнимы и через кнопки Docker Desktop.

## А если я не хочу через Docker...

Чтобы запустить app.py как сервер, напиши в директории [/api-service](/api-service/):

`uvicorn app:app --host 0.0.0.0 --port 8000 --reload`

Чтобы работать Ollama, скачай её [тут](https://ollama.com/).


